x-airflow-common:
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  # image: apache/airflow:latest-python3.8

  &airflow-common
  build:
    context: ./
    dockerfile: ./airflow/airflow.Dockerfile
  image: registry.arfima.com/project-x/project-x-platform:latest
  environment:

    # PG
    &airflow-common-env
    PGSERVICEFILE: /var/run/secrets/pgser

  env_file:
    - ./secrets/airflow/.env

  volumes:
    # Add all the needed folders in airflow
    - ./airflow/dags:/home/airflow/sources/dags
    # - ./secrets/airflow/webserver_config.py:/opt/airflow/webserver_config.py
    - airflow_logs:/home/airflow/sources/logs
    - airflow_plugins:/home/airflow/sources/plugins
    - airflow_tmp_store:/home/airflow/sources/tmp_store
    - /home/docker/rawdata:/home/airflow/sources/rawdata
    - /var/run/docker.sock:/var/run/docker.sock # Share Docker socket

  networks:
    - dtpltbase_dtplt

  secrets:
    - source: pgser

  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy

name: 'dtplt'
services:
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver

    healthcheck:
      test: [ "CMD", "curl", "--fail", "--silent", "--show-error", "http://localhost:8080/baaar/health" ]
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMMAND: "webserver"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
    ports:
      - 8080:8080

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    healthcheck:
      test: [ "CMD-SHELL", 'gosu airflow airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"' ]
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMMAND: "scheduler"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker

    healthcheck:
      test:
        - "CMD-SHELL"
        - 'gosu airflow celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMMAND: "celery worker"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on

    ports:
      - 5688:5678

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    healthcheck:
      test: [ "CMD-SHELL", 'gosu airflow airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"' ]
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMMAND: "triggerer"
    restart: always

  redis:
    image: redis:latest
    container_name: redis
    networks:
      - dtpltbase_dtplt
    volumes:
      - redis_data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  grafana:
    image: grafana/grafana-oss
    container_name: grafana
    ports:
      - '3000:3000'
    env_file:
      - ./secrets/grafana/.envgrafana
    environment:
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/market-dashboard.json
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./provisioning:/etc/grafana/provisioning
      - ./dashboards:/var/lib/grafana/dashboards
    networks:
      - dtpltbase_dtplt
    restart: always

networks:
  dtpltbase_dtplt:
    external: true

secrets:
  pgser:
    file: ./secrets/postgres/dtpltpgs

volumes:
  airflow_logs:
    driver: local
  airflow_plugins:
    driver: local
  airflow_tmp_store:
    driver: local
  redis_data:
    driver: local
  grafana-storage:
    driver: local
